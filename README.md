# ğŸš High-Performance Drone Frame Streaming System

**Real-time frame streaming from Raspberry Pi to Ground Station with 4-8x faster processing!**

---

## ğŸ¯ ADVANCED 3D MESH GENERATION

**NEW: Uses EXACT method from `image_conversion/method2/robust_pipeline.py`**

### Features
- âœ… **SAM (Segment Anything)** - GPU-accelerated foreground masking
- âœ… **COLMAP CUDA** - Sparse/dense reconstruction with GPU  
- âœ… **Open3D Poisson** - High-quality meshing (depth=9, 120K triangles)
- âœ… **GPU Support** - CUDA / DirectML / CPU fallback
- âœ… **Advanced Cleaning** - Plane removal + outlier filtering

### Quick Start

```bash
# 1. Setup (one-time)
setup_advanced_processing.bat

# 2. Download COLMAP CUDA (if needed)
# https://github.com/colmap/colmap/releases
# Extract to: Drone/colmap-x64-windows-cuda/

# 3. Process captured frames
python advanced_mesh_generator.py --session drone_live_1769377285
```

**Output:** `live_sessions/SESSION_NAME/output/final_mesh.stl`

---

## âš¡ Quick Start - Streaming

### 1. Configure Network (30 seconds)

Edit `streaming_config.py`:

```python
GROUND_STATION_IP = "192.168.1.28"  # Change to your Windows PC IP
```

To find your Windows PC IP:

```bash
ipconfig
# Look for "IPv4 Address"
```

---

### 2. Start Ground Station (Windows PC)

**Option A - Easy:**

```bash
start_ground_station.bat
```

**Option B - Manual:**

```bash
pip install flask flask-cors opencv-python numpy
python ground_station_receiver.py
```

You should see:

```
âœ… Running on http://0.0.0.0:5000
```

---

### 3. Start Raspberry Pi Stream

**Option A - Easy:**

```bash
chmod +x start_rpi_stream.sh
./start_rpi_stream.sh
```

**Option B - Manual:**

```bash
pip3 install opencv-python requests numpy
python3 pi_stream_client.py
```

You should see:

```
âœ… Camera ready: 1280x720 @ 30 FPS
âœ… Streaming active!
ğŸ“Š Captured: 120 | Sent: 40 | FPS: 10.2
```

---

## ğŸ“ Output Files

Frames are automatically saved to:

```
live_sessions/
  â””â”€â”€ drone_live_<timestamp>/
      â””â”€â”€ frames/
          â”œâ”€â”€ frame_000001.jpg
          â”œâ”€â”€ frame_000002.jpg
          â””â”€â”€ ...
```

These frames can be processed with RealityScan for 3D reconstruction!

---

## âš™ï¸ Performance Tuning

Edit `streaming_config.py` to optimize for your network:

### For WiFi (Balanced - Default):

```python
FRAME_WIDTH = 1280
FRAME_HEIGHT = 720
JPEG_QUALITY = 85
SKIP_FRAMES = 2
```

**Result:** ~10 FPS, ~3.8 MB/s

### For Maximum Speed (Weak WiFi):

```python
FRAME_WIDTH = 960
FRAME_HEIGHT = 540
JPEG_QUALITY = 75
SKIP_FRAMES = 3
```

**Result:** ~15 FPS, ~1.1 MB/s

### For Best Quality (Ethernet):

```python
FRAME_WIDTH = 1920
FRAME_HEIGHT = 1080
JPEG_QUALITY = 95
SKIP_FRAMES = 0
```

**Result:** ~5-8 FPS, ~30 MB/s

---

## ğŸ§ª Testing

### Test Camera (RPi):

```bash
python3 camera_diagnostic.py
```

This will detect your camera and recommend the best settings.

### Test Connection:

```bash
python test_streaming.py
```

### Calculate Bandwidth:

```bash
python bandwidth_calculator.py
```

---

## ğŸ› Troubleshooting

### "Cannot open camera" or Camera Errors

**Step 1: Run camera diagnostic**

```bash
python3 camera_diagnostic.py
```

**Step 2: Common fixes:**

- âœ… Check devices: `ls /dev/video*`
- âœ… Add user to video group: `sudo usermod -a -G video $USER`
- âœ… For RPi Camera Module:
  - Enable in `sudo raspi-config` â†’ Interface Options â†’ Camera
  - Load driver: `sudo modprobe bcm2835-v4l2`
  - Make permanent: Add `bcm2835-v4l2` to `/etc/modules`
- âœ… For USB camera: Check `lsusb`
- âœ… Reboot after changes: `sudo reboot`

**Step 3: Update config based on diagnostic results**

Edit `streaming_config.py` with recommended CAMERA_ID

### "Connection refused"

- âœ… Make sure ground station is running first
- âœ… Check firewall (allow port 5000)
- âœ… Verify IP in `streaming_config.py`
- âœ… Test: `ping <ground_station_ip>`

### Low FPS / Choppy

- âœ… Increase `SKIP_FRAMES` (3-4)
- âœ… Decrease `JPEG_QUALITY` (70-75)
- âœ… Lower resolution (960x540)
- âœ… Use Ethernet instead of WiFi

### High Latency

- âœ… Reduce `BUFFER_SIZE` (3)
- âœ… Increase `SKIP_FRAMES`
- âœ… Check WiFi signal strength

---

## ğŸ¨ Automatic 3D Mesh Generation

Once frames are captured, automatically generate STL meshes!

### Prerequisites:

1. **Install COLMAP** (CUDA version for GPU acceleration):
   - Download from: https://github.com/colmap/colmap/releases
   - Install to: `C:\Program Files\COLMAP\`
   - Or update path in `auto_process_mesh.py`

2. **Install Python packages:**
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install opencv-python numpy open3d
pip install git+https://github.com/facebookresearch/segment-anything.git
```

3. **Download SAM Model** (optional, for better quality):
   - Download: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
   - Place in Drone folder: `sam_vit_b_01ec64.pth`

### Usage:

**Option 1: Auto-Monitor Mode (Recommended)**
```bash
python auto_process_mesh.py --monitor
```
- Automatically watches `live_sessions/` folder
- Processes new sessions as they complete
- Runs in background

**Option 2: Process Latest Session**
```bash
python auto_process_mesh.py
```

**Option 3: Process Specific Session**
```bash
python auto_process_mesh.py --session drone_live_1737843600
```

### Output:

```
live_sessions/
  â””â”€â”€ drone_live_<timestamp>/
      â”œâ”€â”€ frames/              # Original captured frames
      â”œâ”€â”€ processing/          # Intermediate files
      â”‚   â”œâ”€â”€ masked/         # SAM-processed frames
      â”‚   â””â”€â”€ colmap/         # COLMAP workspace
      â””â”€â”€ output/
          â”œâ”€â”€ dense.ply       # Dense point cloud
          â””â”€â”€ final_mesh.stl  # Final 3D mesh! ğŸ‰
```

### Performance:

| Frames | Processing Time (RTX GPU) |
|--------|---------------------------|
| 50-75  | ~5-8 minutes             |
| 100    | ~10-12 minutes           |
| 150+   | ~15-20 minutes           |

---

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RASPBERRY PI    â”‚                  â”‚ GROUND STATION   â”‚
â”‚ (Drone)         â”‚                  â”‚ (Windows PC)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   WiFi/Ethernet  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Capture      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚ 1. Receive       â”‚
â”‚ 2. Compress     â”‚   Compressed     â”‚ 2. Decompress    â”‚
â”‚    (5-10ms)     â”‚   JPEGs          â”‚    (5ms)         â”‚
â”‚ 3. Send         â”‚  (~500KB-4MB/s)  â”‚ 3. Save frames   â”‚
â”‚                 â”‚                  â”‚ 4. AUTO-PROCESS  â”‚
â”‚ CPU: 15-25%     â”‚                  â”‚    â†’ STL Mesh!   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why This Design?**

- âœ… 4-8x FASTER than processing on RPi
- âœ… RPi stays cool (15-25% CPU vs 85-95%)
- âœ… Ground station has 20x more power
- âœ… Real-time latency (60-90ms)

---

## ğŸ“ˆ Performance Metrics

| Metric      | This System     | RPi Processing |
| ----------- | --------------- | -------------- |
| **FPS**     | **11-16** âš¡    | 1.5-3 ğŸŒ       |
| **Latency** | **60-90ms**     | 330-680ms      |
| **RPi CPU** | **15-25%**      | 85-95%         |
| **Speed**   | **4-8x FASTER** | 1x             |

---

## ğŸ”— Integration with RealityScan

To process captured frames with RealityScan:

```bash
python realityscan_align.py <path_to_frames_folder>
```

Example:

```bash
python realityscan_align.py live_sessions/drone_live_1737843600/frames
```

---

## ğŸ“‚ Project Files

### Core System:

- `pi_stream_client.py` - RPi camera capture & streaming
- `ground_station_receiver.py` - Windows receiver & processor
- `streaming_config.py` - Configuration (edit this!)

### Launchers:

### Launchers:

- `start_ground_station.bat` - Windows quick start
- `start_rpi_stream.sh` - RPi quick start

### 3D Mesh Generation:

- `auto_process_mesh.py` - **NEW!** Automatic STL mesh generator
- Download SAM model: `sam_vit_b_01ec64.pth`
- Install COLMAP (CUDA version recommended)

### Utilities:

- `camera_diagnostic.py` - Camera detection & troubleshooting
- `test_streaming.py` - Diagnostic tests
- `bandwidth_calculator.py` - Network calculator

### Legacy (Old Batch System):

- `windows_server_api.py` - RealityScan processing server
- `realityscan_align.py` - 3D reconstruction script
- `pi_client_api.py` - Batch upload (old system)

---

## ğŸ’¡ Tips

1. **Use Ethernet** when possible for best performance
2. **Monitor stats** on both RPi and ground station
3. **Adjust settings** based on your network quality
4. **Run auto-processor** in background with `--monitor` flag
5. **Multiple drones** supported (unique session IDs)
6. **Need 50+ frames** for good 3D reconstruction

---

## âœ… Success Criteria

You're ready when:

- âœ… Ground station shows "New session" message
- âœ… RPi shows "Streaming active" with FPS counter
- âœ… Frames are saving to `live_sessions/` folder
- âœ… CPU usage on RPi is low (15-25%)
- âœ… Auto-processor generates STL mesh
- âœ… No error messages

---

## ğŸš€ Complete Workflow

### Quick Start (5 minutes):
1. Configure IP in `streaming_config.py`
2. Start ground station: `start_ground_station.bat`
3. Start auto-processor: `python auto_process_mesh.py --monitor`
4. Start RPi stream: `./start_rpi_stream.sh`
5. Fly drone and capture!

### Full Pipeline (Automatic):
```
Drone captures â†’ Stream to ground station â†’ Auto-save frames
                                          â†“
                              Auto-detect new session
                                          â†“
                              SAM masking + COLMAP processing
                                          â†“
                              Generate STL mesh (10-15 min)
                                          â†“
                              final_mesh.stl ready! ğŸ‰
```

**From flight to 3D mesh: Fully automated!**

---

**Happy Flying! ğŸšâœ¨**
